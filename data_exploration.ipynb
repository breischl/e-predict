{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download historical data from EIA and NOAA/GHCN-d to the local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_historical_data as dl\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "HISTORICAL_DATA_DIR = os.path.abspath(\"./historical_data\")\n",
    "ANALYSIS_DATA_DIR = os.path.abspath(\"./analysis_data/\")\n",
    "ELECTRIC_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"electric_data\")\n",
    "WEATHER_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"weather_station_data\")\n",
    "\n",
    "for dir in [HISTORICAL_DATA_DIR, ANALYSIS_DATA_DIR, ELECTRIC_DATA_DIR, WEATHER_DATA_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "WEATHER_STATION_IDS = [\n",
    "    \"USW00023066\",  # Grand Junction Walker Field\n",
    "    \"USC00053553\",  # Greeley UNC\n",
    "    \"USC00053005\",  # Ft Collins\n",
    "    \"USC00050848\",  # Boulder\n",
    "    \"USC00055984\",  # Northglenn\n",
    "    \"USC00058995\",  # Wheat Ridge\n",
    "    \"USW00023061\"  # Alamosa\n",
    "]\n",
    "\n",
    "# Uncomment to force re-download of source data\n",
    "# Otherwise can also run the download script manually via: python download_historical_data.py\n",
    "# Data files are saved locally so you only need to re-download to get new/different data\n",
    "#dl.download_eia_historical_data(ELECTRIC_DATA_DIR, eia_respondent=\"PSCO\")\n",
    "#dl.download_ghcnd_historical_data(WEATHER_DATA_DIR, WEATHER_STATION_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "plt.style.use(\"default\") #alternative \"ggplot\"\n",
    "\n",
    "temp_df : pd.DataFrame = None\n",
    "\n",
    "## Load up temperature data for each weather station, into their own columns\n",
    "for df_file in glob.glob(WEATHER_DATA_DIR + \"\\*.json\"):\n",
    "    with open(df_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        station_id = os.path.basename(df_file)[0:11]\n",
    "        station_df = pd.read_json(f)\n",
    "        station_df.index.rename(\"date\", inplace=True)\n",
    "        \n",
    "        # TODO: This name-mangling seems like a halfassed way to either do a MultiIndex or maybe a tuple-index\n",
    "        # Going to leave it for now as I'm not clear what will be easiest when trying to train an ML model\n",
    "        col_renames = {col: f\"{station_id}_{col}\" for col in station_df.columns}\n",
    "        station_df.rename(col_renames, axis=\"columns\", inplace=True)\n",
    "        \n",
    "        if temp_df is not None:\n",
    "            temp_df = pd.merge(left=temp_df, right=station_df, how=\"outer\", left_index=True, right_index=True)\n",
    "        else:\n",
    "            temp_df = station_df\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(4)\n",
    "ax1.plot(temp_df)\n",
    "_ = ax2.hist(temp_df, bins=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load electric demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psco_demand_data_file = os.path.join(ELECTRIC_DATA_DIR, \"psco-daily-dataframe.json\")\n",
    "with open(psco_demand_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics on daily demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_df[\"daily_demand\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(4)\n",
    "ax1.plot(demand_df)\n",
    "ax2.hist(demand_df, bins=50)\n",
    "pd.plotting.autocorrelation_plot(demand_df, ax=ax3).set_xlim([0,365])\n",
    "pd.plotting.autocorrelation_plot(demand_df, ax=ax4).set_xlim([0,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(demand_df, temp_df, how=\"outer\", left_index=True, right_index=True)\n",
    "joined_df.dropna(inplace=True)\n",
    "\n",
    "## Graphs demand vs tmin & max, which is harder with many weather stations of data\n",
    "# slice = joined_df[[\"daily_demand\", \"tmax\", \"tmin\"]]  #[\"2016-12-01\":\"2017-03-01\"]\n",
    "\n",
    "# fig, ax1 = plt.subplots(1, 1)\n",
    "# fig.set_figwidth(15)\n",
    "# fig.set_figheight(4)\n",
    "# ax1.set_ylabel(\"megawatt-hours\")\n",
    "# ax1.plot(slice[\"daily_demand\"], color=\"tab:green\")\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.set_ylabel(\"deg C\")\n",
    "# ax2.plot(slice[\"tmax\"], color=\"tab:red\")\n",
    "# ax2.plot(slice[\"tmin\"], color=\"tab:blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.corr(numeric_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-predict",
   "language": "python",
   "name": "e-predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbc800b9954356ddf51c3c2e08059eab8570ab1b3611f475161bbcfb4053c97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
