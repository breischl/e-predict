{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import download_historical_data as dl\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing some stuff from the FastAI book\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "import fastai.tabular.all as aiTab\n",
    "import sklearn as sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "import dtreeviz.trees as dtrees\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "\n",
    "import math\n",
    "\n",
    "# Functions to check root-mean-squared error for the model\n",
    "def r_mse(pred, y): return round(math.sqrt(((pred - y)**2).mean()), 6)\n",
    "def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n",
    "def r_mape(pred, actual): return round(sklearn.metrics.mean_absolute_percentage_error(actual, pred), 5)\n",
    "def m_mape(m, xs, y): return r_mape(m.predict(xs), y)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.max_columns = 6\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "HISTORICAL_DATA_DIR = os.path.abspath(\"./historical_data\")\n",
    "ANALYSIS_DATA_DIR = os.path.abspath(\"./analysis_data/\")\n",
    "ELECTRIC_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"electric_data\")\n",
    "WEATHER_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"weather_station_data\")\n",
    "\n",
    "for dir in [HISTORICAL_DATA_DIR, ANALYSIS_DATA_DIR, ELECTRIC_DATA_DIR, WEATHER_DATA_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "WEATHER_STATION_IDS = [\n",
    "    \"USW00023066\",  # Grand Junction Walker Field\n",
    "    \"USC00053553\",  # Greeley UNC\n",
    "    \"USC00053005\",  # Ft Collins\n",
    "    \"USC00050848\",  # Boulder\n",
    "    \"USC00055984\",  # Northglenn\n",
    "    \"USC00058995\",  # Wheat Ridge\n",
    "    \"USW00023061\"  # Alamosa\n",
    "]\n",
    "\n",
    "# Uncomment following lines to force re-download of source data\n",
    "# Otherwise can also run the download script manually via: python download_historical_data.py\n",
    "# Data files are saved locally so you only need to re-download to get new/different data\n",
    "\n",
    "#dl.download_eia_historical_data(ELECTRIC_DATA_DIR, eia_respondent=\"PSCO\")\n",
    "#dl.download_ghcnd_historical_data(WEATHER_DATA_DIR, WEATHER_STATION_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "plt.style.use(\"default\") #alternative \"ggplot\"\n",
    "\n",
    "temp_df : pd.DataFrame = None\n",
    "\n",
    "## Load up temperature data for each weather station, into their own columns\n",
    "for df_file in glob.glob(WEATHER_DATA_DIR + \"\\*.json\"):\n",
    "    with open(df_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        station_id = os.path.basename(df_file)[0:11]\n",
    "        station_df = pd.read_json(f)\n",
    "        station_df.index.rename(\"date\", inplace=True)\n",
    "        \n",
    "        # TODO: This name-mangling seems like a halfassed way to either do a MultiIndex or maybe a tuple-index\n",
    "        # Going to leave it for now as I'm not clear what will be easiest when trying to train an ML model\n",
    "        col_renames = {col: f\"{station_id}_{col}\" for col in station_df.columns}\n",
    "        station_df.rename(col_renames, axis=\"columns\", inplace=True)\n",
    "        \n",
    "        if temp_df is not None:\n",
    "            temp_df = pd.merge(left=temp_df, right=station_df, how=\"outer\", left_index=True, right_index=True)\n",
    "        else:\n",
    "            temp_df = station_df\n",
    "\n",
    "len(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PSCO electric demand data from EIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psco_demand_data_file = os.path.join(ELECTRIC_DATA_DIR, \"psco-daily-dataframe.json\")\n",
    "with open(psco_demand_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_df = pd.read_json(f)\n",
    "\n",
    "len(demand_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge demand and temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(demand_df, temp_df, how=\"outer\", left_index=True, right_index=True)\n",
    "joined_df.dropna(inplace=True)\n",
    "#len(joined_df),joined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data with new dates and maybe some other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment data\n",
    "augmented_df = joined_df.copy()\n",
    "\n",
    "# Extract date index into a column\n",
    "augmented_df.reset_index(inplace=True)\n",
    "augmented_df[\"date\"] = augmented_df[\"index\"]  \n",
    "augmented_df.set_index(\"index\", inplace=True)\n",
    "\n",
    "## Adds date parts\n",
    "augmented_df = aiTab.add_datepart(augmented_df, \"date\", drop=True)\n",
    "## But a lot of the augmented parts are not that applicable in our case\n",
    "augmented_df.drop(['Elapsed', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n",
    "                   'Is_year_end', 'Is_year_start'], axis=1, inplace=True)\n",
    "\n",
    "## Add lagged values\n",
    "for lag in range(1, 15):\n",
    "    augmented_df[f\"demand_lag_{lag}\"] = augmented_df[\"daily_demand\"].shift(lag)\n",
    "augmented_df.dropna(inplace=True) ## Lag columns will have NaN values\n",
    "\n",
    "# Create masks for data sets. Need to do this before we drop the non-preductive columns\n",
    "# Because the \"Year\" columns will be dropped\n",
    "train_mask = (augmented_df.Year < 2021)\n",
    "validation_mask = ((augmented_df.Year >= 2021) & (augmented_df.Year < 2022))\n",
    "test_mask = (augmented_df.Year >= 2022)\n",
    "\n",
    "# List of columns with more predictive value, extracted from random_forest.ipynb\n",
    "cols_to_keep = ['daily_demand', 'demand_lag_1', 'demand_lag_7', 'demand_lag_3', 'Dayofweek',\n",
    "                'USC00050848_tmax', 'USC00055984_tmax', 'demand_lag_4',\n",
    "                'USC00053553_tmax', 'demand_lag_14', 'USC00053553_tmin', 'demand_lag_8',\n",
    "                'USW00023061_tmax']\n",
    "\n",
    "# Drop non-predictive columns\n",
    "augmented_df = augmented_df.filter(items=cols_to_keep, axis='columns')\n",
    "\n",
    "augmented_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out training, test and validation sets\n",
    "train_df = augmented_df.where(train_mask).dropna()\n",
    "validation_df = augmented_df.where(validation_mask).dropna()\n",
    "test_df = augmented_df.where(test_mask).dropna()\n",
    "\n",
    "train_idx = np.where(train_mask)[0]\n",
    "valid_idx = np.where(validation_mask)[0]\n",
    "test_idx = np.where(test_mask)[0]\n",
    "print(f\"trainSize={len(train_idx)}, validationSize={len(valid_idx)}, testSize={len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick out the validation input and output data (and duplicate that effect with the test Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = train_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "y = train_df[\"daily_demand\"]\n",
    "valid_xs = validation_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "valid_y = validation_df[\"daily_demand\"]\n",
    "test_xs = test_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "test_y = test_df[\"daily_demand\"]\n",
    "full_xs = augmented_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "full_y = augmented_df[\"daily_demand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using aiTab.Categorify from the book\n",
    "procs_nn = [aiTab.FillMissing, aiTab.Normalize]\n",
    "\n",
    "splits = (list(train_idx), list(valid_idx))\n",
    "\n",
    "tab_panda = aiTab.TabularPandas(augmented_df, procs_nn, cat_names=[], cont_names=list(xs.columns),\n",
    "                            splits=splits, y_names=\"daily_demand\")\n",
    "\n",
    "dataloader = tab_panda.dataloaders(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dependent variable min/max\n",
    "y = tab_panda.train.y\n",
    "y.min(),y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = aiTab.tabular_learner(dataloader, y_range=(90000, 200000), layers=[500, 250],\n",
    "                        n_out=1, loss_func=aiTab.F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(5, 0.00039)\n",
    "learn.fit(25, 0.00039)\n",
    "\n",
    "preds, targets = learn.get_preds()\n",
    "r_mape(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check errors on our test set\n",
    "#prediction, bias, contributions = treeinterpreter.predict(forest, row.values)\n",
    "# m_mape(learn.predict(test_xs[0:0]), test_xs, test_y)\n",
    "# learn.predict(test_xs[0])\n",
    "test_xs[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot predicted vs actual\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(4)\n",
    "ax1.plot(pred_df)\n",
    "ax2.plot(error_df)\n",
    "ax3.hist(error_df, bins=25)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.plot.scatter(x=\"actual_demand\", y=\"predicted_demand\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44b48c3bcf9419f2eb3f4f9835c1f06c11c1fdf53404b3684d945f8ae454e88b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
