{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download historical data from EIA and NOAA/GHCN-d to the local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_historical_data as dl\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing some stuff from the FastAI book\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "import fastai.tabular.all as aiTab\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "import dtreeviz.trees as dtrees\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.max_columns = 6\n",
    "\n",
    "HISTORICAL_DATA_DIR = os.path.abspath(\"./historical_data\")\n",
    "ANALYSIS_DATA_DIR = os.path.abspath(\"./analysis_data/\")\n",
    "ELECTRIC_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"electric_data\")\n",
    "WEATHER_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"weather_station_data\")\n",
    "\n",
    "for dir in [HISTORICAL_DATA_DIR, ANALYSIS_DATA_DIR, ELECTRIC_DATA_DIR, WEATHER_DATA_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "WEATHER_STATION_IDS = [\n",
    "    \"USW00023066\",  # Grand Junction Walker Field\n",
    "    \"USC00053553\",  # Greeley UNC\n",
    "    \"USC00053005\",  # Ft Collins\n",
    "    \"USC00050848\",  # Boulder\n",
    "    \"USC00055984\",  # Northglenn\n",
    "    \"USC00058995\",  # Wheat Ridge\n",
    "    \"USW00023061\"  # Alamosa\n",
    "]\n",
    "\n",
    "# Uncomment following lines to force re-download of source data\n",
    "# Otherwise can also run the download script manually via: python download_historical_data.py\n",
    "# Data files are saved locally so you only need to re-download to get new/different data\n",
    "\n",
    "#dl.download_eia_historical_data(ELECTRIC_DATA_DIR, eia_respondent=\"PSCO\")\n",
    "#dl.download_ghcnd_historical_data(WEATHER_DATA_DIR, WEATHER_STATION_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the weather DataFrame\n",
    "temp_df = dl.read_weather_data(WEATHER_DATA_DIR, WEATHER_STATION_IDS, earliest_date=None)\n",
    "len(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PSCO electric demand data from EIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psco_demand_data_file = os.path.join(ELECTRIC_DATA_DIR, \"psco-daily-dataframe.json\")\n",
    "with open(psco_demand_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_df = pd.read_json(f)\n",
    "\n",
    "len(demand_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge demand and temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(demand_df, temp_df, how=\"outer\", left_index=True, right_index=True)\n",
    "joined_df.dropna(inplace=True)\n",
    "len(joined_df),joined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data with new dates and maybe some other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment data\n",
    "augmented_df = joined_df.copy()\n",
    "\n",
    "# Extract date index into a column\n",
    "augmented_df.reset_index(inplace=True)\n",
    "augmented_df[\"date\"] = augmented_df[\"index\"]  \n",
    "augmented_df.set_index(\"index\", inplace=True)\n",
    "\n",
    "## Adds date parts\n",
    "augmented_df = aiTab.add_datepart(augmented_df, \"date\", drop=True)\n",
    "## But a lot of the augmented parts are not that applicable in our case\n",
    "augmented_df.drop(['Elapsed', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n",
    "                   'Is_year_end', 'Is_year_start'], axis=1, inplace=True)\n",
    "\n",
    "## Add lagged values\n",
    "for lag in range(1, 15):\n",
    "    augmented_df[f\"demand_lag_{lag}\"] = augmented_df[\"daily_demand\"].shift(lag)\n",
    "augmented_df.dropna(inplace=True) ## Lag columns will have NaN values\n",
    "\n",
    "\n",
    "augmented_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for data sets\n",
    "train_mask = (augmented_df.Year < 2021)\n",
    "validation_mask = ((augmented_df.Year >= 2021) & (augmented_df.Year < 2022))\n",
    "test_mask = (augmented_df.Year >= 2022)\n",
    "\n",
    "## Split out training, test and validation sets\n",
    "train_df = augmented_df.where(train_mask).dropna()\n",
    "validation_df = augmented_df.where(validation_mask).dropna()\n",
    "test_df = augmented_df.where(test_mask).dropna()\n",
    "\n",
    "# print(train_df.iloc[0:5][[\"daily_demand\"]])\n",
    "# print(validation_df.iloc[0:5][[\"daily_demand\"]])\n",
    "# print(test_df.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create indexes from the sets\n",
    "train_idx = np.where(train_mask)[0]\n",
    "valid_idx = np.where(validation_mask)[0]\n",
    "test_idx = np.where(test_mask)[0]\n",
    "print(f\"trainSize={len(train_idx)}, validationSize={len(valid_idx)}, testSize={len(test_idx)}\")\n",
    "# print(train_idx[0:5])\n",
    "# print(valid_idx[0:5])\n",
    "# print(test_idx[0:5])\n",
    "\n",
    "splits = (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out categorical vs continuous data\n",
    "cont, cat = aiTab.cont_cat_split(augmented_df, 1, dep_var=\"daily_demand\")\n",
    "print(cont)\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create TabularPandas\n",
    "procs = [aiTab.Categorify, aiTab.FillMissing]\n",
    "dep_var = \"daily_demand\"\n",
    "to = aiTab.TabularPandas(augmented_df, procs, cat, cont, y_names=dep_var, splits=splits)\n",
    "len(to.train),len(to.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick out the validation input and output data (and duplicate that effect with the test Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,y = to.train.xs,to.train.y\n",
    "valid_xs,valid_y = to.valid.xs,to.valid.y\n",
    "test_xs = test_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "test_y = test_df[\"daily_demand\"]\n",
    "full_xs = augmented_df.drop(\"daily_demand\", axis=1, inplace=False)\n",
    "full_y = augmented_df[\"daily_demand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a decision tree\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "_ = tree.fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import re\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):\n",
    "    s = export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n",
    "                        special_characters=True, rotate=False, precision=precision, **kwargs)\n",
    "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n",
    "\n",
    "\n",
    "# Draw the tree\n",
    "draw_tree(tree, xs, size=10, leaves_parallel=True, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see that in DTreeViz\n",
    "samp_idx = np.random.permutation(len(y))[:500]\n",
    "dtrees.dtreeviz(tree, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n",
    "         fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n",
    "         orientation='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOAR LEAVES\n",
    "tree = DecisionTreeRegressor(min_samples_leaf=25)\n",
    "_ = tree.fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to check root-mean-squared error for the model\n",
    "import math\n",
    "def r_mse(pred, y): return round(math.sqrt(((pred - y)**2).mean()), 6)\n",
    "def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check error in the training and validation set\n",
    "m_rmse(tree, xs, y), m_rmse(tree, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many leaves do we have, vs number of measurements? Checking for overfitting here.\n",
    "tree.get_n_leaves(), len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the full tree (use carefully, it's pretty big)\n",
    "# samp_idx = np.random.permutation(len(y))[:500]\n",
    "# dtrees.dtreeviz(tree, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n",
    "#                 fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n",
    "#                 orientation='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph test set predictions vs actuals\n",
    "y_pred = tree.predict(test_xs)\n",
    "pred_df = pd.DataFrame(data=y_pred, index=list(test_xs.index))\n",
    "pred_df = pred_df.join(test_y, how=\"inner\")\n",
    "pred_df.rename(columns={0: \"predicted_demand\", \"daily_demand\":\"actual_demand\"}, inplace=True)\n",
    "pred_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph full dataset predictions vs actuals\n",
    "y_pred = tree.predict(full_xs)\n",
    "pred_df = pd.DataFrame(data=y_pred, index=list(full_xs.index))\n",
    "pred_df = pred_df.join(full_y, how=\"inner\")\n",
    "pred_df.rename(columns={0: \"predicted_demand\", \"daily_demand\": \"actual_demand\"}, inplace=True)\n",
    "error_df = pred_df[\"actual_demand\"] - pred_df[\"predicted_demand\"]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(4)\n",
    "ax1.plot(pred_df)\n",
    "ax2.plot(error_df)\n",
    "ax3.hist(error_df, bins=25)\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-predict",
   "language": "python",
   "name": "e-predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbc800b9954356ddf51c3c2e08059eab8570ab1b3611f475161bbcfb4053c97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
