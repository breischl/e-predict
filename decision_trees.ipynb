{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download historical data from EIA and NOAA/GHCN-d to the local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_historical_data as dl\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing some stuff from the FastAI book\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "import fastai.tabular.all as aiTab\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "import dtreeviz.trees as dtrees\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8\n",
    "\n",
    "HISTORICAL_DATA_DIR = os.path.abspath(\"./historical_data\")\n",
    "ANALYSIS_DATA_DIR = os.path.abspath(\"./analysis_data/\")\n",
    "ELECTRIC_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"electric_data\")\n",
    "WEATHER_DATA_DIR = os.path.join(HISTORICAL_DATA_DIR, \"weather_station_data\")\n",
    "\n",
    "for dir in [HISTORICAL_DATA_DIR, ANALYSIS_DATA_DIR, ELECTRIC_DATA_DIR, WEATHER_DATA_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "WEATHER_STATION_IDS = [\n",
    "    \"USW00023066\",  # Grand Junction Walker Field\n",
    "    \"USC00053553\",  # Greeley UNC\n",
    "    \"USC00053005\",  # Ft Collins\n",
    "    \"USC00050848\",  # Boulder\n",
    "    \"USC00055984\",  # Northglenn\n",
    "    \"USC00058995\",  # Wheat Ridge\n",
    "    \"USW00023061\"  # Alamosa\n",
    "]\n",
    "\n",
    "# Uncomment to force re-download of source data\n",
    "# Otherwise can also run the download script manually via: python download_historical_data.py\n",
    "# Data files are saved locally so you only need to re-download to get new/different data\n",
    "#dl.download_eia_historical_data(ELECTRIC_DATA_DIR, eia_respondent=\"PSCO\")\n",
    "#dl.download_ghcnd_historical_data(WEATHER_DATA_DIR, WEATHER_STATION_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "plt.style.use(\"default\") #alternative \"ggplot\"\n",
    "\n",
    "temp_df : pd.DataFrame = None\n",
    "\n",
    "## Load up temperature data for each weather station, into their own columns\n",
    "for df_file in glob.glob(WEATHER_DATA_DIR + \"\\*.json\"):\n",
    "    with open(df_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        station_id = os.path.basename(df_file)[0:11]\n",
    "        station_df = pd.read_json(f)\n",
    "        station_df.index.rename(\"date\", inplace=True)\n",
    "        \n",
    "        # TODO: This name-mangling seems like a halfassed way to either do a MultiIndex or maybe a tuple-index\n",
    "        # Going to leave it for now as I'm not clear what will be easiest when trying to train an ML model\n",
    "        col_renames = {col: f\"{station_id}_{col}\" for col in station_df.columns}\n",
    "        station_df.rename(col_renames, axis=\"columns\", inplace=True)\n",
    "        \n",
    "        if temp_df is not None:\n",
    "            temp_df = pd.merge(left=temp_df, right=station_df, how=\"outer\", left_index=True, right_index=True)\n",
    "        else:\n",
    "            temp_df = station_df\n",
    "\n",
    "# station_df[\"tmp_date\"] = station_df.index\n",
    "# station_df[\"day_of_year\"] = station_df[\"tmp_date\"].dt.day_of_year\n",
    "# station_df.drop(\"tmp_date\", axis=1, inplace=True)\n",
    "\n",
    "len(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load electric demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psco_demand_data_file = os.path.join(ELECTRIC_DATA_DIR, \"psco-daily-dataframe.json\")\n",
    "with open(psco_demand_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_df = pd.read_json(f)\n",
    "\n",
    "len(demand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(demand_df, temp_df, how=\"outer\", left_index=True, right_index=True)\n",
    "joined_df.dropna(inplace=True)\n",
    "len(joined_df),joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augment data\n",
    "augmented_df = joined_df.copy()\n",
    "\n",
    "# Extract date index into a column\n",
    "augmented_df.reset_index(inplace=True)\n",
    "augmented_df[\"date\"] = augmented_df[\"index\"]  \n",
    "augmented_df.set_index(\"index\", inplace=True)\n",
    "\n",
    "augmented_df = aiTab.add_datepart(augmented_df, \"date\", drop=True)\n",
    "## A lot of the augmented parts are not that applicable in our case\n",
    "augmented_df.drop(['Elapsed', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n",
    "                   'Is_year_end', 'Is_year_start'], axis=1, inplace=True)\n",
    "augmented_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out training, test and validation sets\n",
    "# train = augmented_df.loc[:\"2021-01-01\"]\n",
    "# validation = augmented_df.loc[\"2021-01-01\":\"2022-01-01\"]\n",
    "# test = augmented_df.loc[\"2022-01-01\":]\n",
    "train_idx = np.where(augmented_df.Year < 2021)[0]\n",
    "valid_idx = np.where((augmented_df.Year >= 2021) & (augmented_df.Year < 2022))[0]\n",
    "test_idx = np.where(augmented_df.Year >= 2022)[0]\n",
    "print(f\"trainSize={len(train_idx)}, validationSize={len(valid_idx)}, testSize={len(test_idx)}\")\n",
    "\n",
    "splits = (list(train_idx), list(valid_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out categorical vs continuous data\n",
    "cont, cat = aiTab.cont_cat_split(augmented_df, 1, dep_var=\"daily_demand\")\n",
    "print(cont)\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create TabularPandas\n",
    "procs = [aiTab.Categorify, aiTab.FillMissing]\n",
    "dep_var = \"daily_demand\"\n",
    "to = aiTab.TabularPandas(augmented_df, procs, cat, cont, y_names=dep_var, splits=splits)\n",
    "len(to.train),len(to.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiTab.save_pickle(os.path.join(ANALYSIS_DATA_DIR, 'tabular.pkl'), to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload from pickle\n",
    "to = aiTab.load_pickle(os.path.join(ANALYSIS_DATA_DIR, 'tabular.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,y = to.train.xs,to.train.y\n",
    "valid_xs,valid_y = to.valid.xs,to.valid.y\n",
    "xs, y, valid_xs, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a decision tree\n",
    "m = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "m.fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import re\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):\n",
    "    s = export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n",
    "                        special_characters=True, rotate=False, precision=precision, **kwargs)\n",
    "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n",
    "\n",
    "\n",
    "# Draw the tree\n",
    "draw_tree(m, xs, size=10, leaves_parallel=True, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see that in DTreeViz\n",
    "samp_idx = np.random.permutation(len(y))[:500]\n",
    "dtrees.dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n",
    "         fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n",
    "         orientation='LR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOAR LEAVES\n",
    "m = DecisionTreeRegressor(min_samples_leaf=15)\n",
    "m.fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to check root-mean-squared error for the model\n",
    "import math\n",
    "def r_mse(pred, y): return round(math.sqrt(((pred - y)**2).mean()), 6)\n",
    "def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check error in the training set\n",
    "m_rmse(m, xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check RMS error against validation set\n",
    "m_rmse(m, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many leaves do we have? Oh, it's one per measurement, so massively overfitted\n",
    "m.get_n_leaves(), len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the tree (use carefully, it's pretty big)\n",
    "# samp_idx = np.random.permutation(len(y))[:500]\n",
    "# dtrees.dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n",
    "#                 fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n",
    "#                 orientation='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to grow a random forest with some default parameters chosen\n",
    "\n",
    "## n_estimators -> number of trees in the forest\n",
    "num_estimators = 200\n",
    "\n",
    "def grow_random_forest(xs, y, n_estimators=num_estimators, max_samples=0.8,\n",
    "       max_features=0.5, min_samples_leaf=4, **kwargs):\n",
    "    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n",
    "                                 max_samples=max_samples, max_features=max_features,\n",
    "                                 min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = grow_random_forest(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rmse(forest, xs, y), m_rmse(forest, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions from each individual tree in the forest\n",
    "preds = np.stack([t.predict(valid_xs) for t in forest.estimators_])\n",
    "\n",
    "## Plot the mean error for a given number of estimators used\n",
    "plt.plot([r_mse(preds[:i + 1].mean(0), valid_y) for i in range(num_estimators)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-bag errors for the forest\n",
    "r_mse(forest.oob_prediction_, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-predict",
   "language": "python",
   "name": "e-predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbc800b9954356ddf51c3c2e08059eab8570ab1b3611f475161bbcfb4053c97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
